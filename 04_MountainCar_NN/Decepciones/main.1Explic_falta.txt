 

 500 episodios

 La funcion llamada ourReward recibe un estado y una 
 penalizacion y devuelve una recompensa nueva para 
 sustituir la que devuelve el entorno

 La nueva recompensa se basa en la 
 DISTANCIA ABSOLUTA RESPECTO AL CENTRO, 
 siendo el centro como 0, por lo que el objetivo 
 se encuentra en la porsicion 1(y no en la 0.5 como 
 plante al entorno)

 Además se dara una bonificacion de +1 
 si la velocidad es positiva y se ecuentra
 por encima de 0.5.

 por ultimo se aplica ua penalizacion del 0.005% 
 por cada episodio que el agente haya realizado 
 a la recompensa
 
RESULTADO: CapturaCambiandoRewardAPosicion.PNG

 NOTA: parece que con 0.995 es demasiada penalizacion, 
 se puede ver como la recompensa converge a cierto punto y 
 se debe a que a medida que abanza la penalizacion se vuelve 
 demasiado grande: [reward1 * 0.995, reward2 * 0.995^2,... reward200 * 0.36695782172]
 probar con 0.998-0.999(0.670051 - 0.818648) daria penalizaciones un poco más asequibles para el modelo

RESULTADO CON MEJORA: main.1.PenalizacionA0.999_0wins.png

 Ciertamente no ha conseguido ni un solo win... 
 usando la reward del entrono aprende a resolver 
 el problema entorno a los 1200 episodios,  
 vemos en cuantos lo hace este: